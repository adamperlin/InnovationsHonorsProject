{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/mnist/train-images-idx3-ubyte.gz\n",
      "Extracting data/mnist/train-labels-idx1-ubyte.gz\n",
      "Extracting data/mnist/t10k-images-idx3-ubyte.gz\n",
      "Extracting data/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Average loss epoch 0: 0.38129432218058124\n",
      "Average loss epoch 1: 0.2983722120419249\n",
      "Average loss epoch 2: 0.28675883944148506\n",
      "Average loss epoch 3: 0.27799071703271155\n",
      "Average loss epoch 4: 0.2764744143281783\n",
      "Average loss epoch 5: 0.27268220821182765\n",
      "Average loss epoch 6: 0.2707376445715244\n",
      "Average loss epoch 7: 0.2693529358450627\n",
      "Average loss epoch 8: 0.2666008232947274\n",
      "Average loss epoch 9: 0.26454374529334496\n",
      "Average loss epoch 10: 0.26355784441971836\n",
      "Average loss epoch 11: 0.263056271933106\n",
      "Average loss epoch 12: 0.2638347000925691\n",
      "Average loss epoch 13: 0.2585764169727728\n",
      "Average loss epoch 14: 0.2598707673502413\n",
      "Average loss epoch 15: 0.2600754579672447\n",
      "Average loss epoch 16: 0.26073519329785744\n",
      "Average loss epoch 17: 0.25554844216033296\n",
      "Average loss epoch 18: 0.2561011465314107\n",
      "Average loss epoch 19: 0.2557938626700348\n",
      "Average loss epoch 20: 0.2524923681865483\n",
      "Average loss epoch 21: 0.25598613777152307\n",
      "Average loss epoch 22: 0.254561090212324\n",
      "Average loss epoch 23: 0.2537708527002579\n",
      "Average loss epoch 24: 0.2539336468268941\n",
      "Average loss epoch 25: 0.25263675561040155\n",
      "Average loss epoch 26: 0.25225776267218425\n",
      "Average loss epoch 27: 0.2527089975340105\n",
      "Average loss epoch 28: 0.2503275146017541\n",
      "Average loss epoch 29: 0.2534629994512716\n",
      "Total time: 17.785855293273926 seconds\n",
      "Optimization Finished!\n",
      "119.0\n",
      "121.0\n",
      "120.0\n",
      "114.0\n",
      "120.0\n",
      "121.0\n",
      "112.0\n",
      "116.0\n",
      "120.0\n",
      "119.0\n",
      "116.0\n",
      "117.0\n",
      "121.0\n",
      "116.0\n",
      "116.0\n",
      "117.0\n",
      "115.0\n",
      "116.0\n",
      "121.0\n",
      "123.0\n",
      "119.0\n",
      "114.0\n",
      "118.0\n",
      "116.0\n",
      "118.0\n",
      "119.0\n",
      "117.0\n",
      "119.0\n",
      "119.0\n",
      "114.0\n",
      "120.0\n",
      "116.0\n",
      "118.0\n",
      "120.0\n",
      "119.0\n",
      "121.0\n",
      "115.0\n",
      "113.0\n",
      "113.0\n",
      "118.0\n",
      "119.0\n",
      "118.0\n",
      "119.0\n",
      "114.0\n",
      "119.0\n",
      "118.0\n",
      "119.0\n",
      "122.0\n",
      "121.0\n",
      "121.0\n",
      "119.0\n",
      "119.0\n",
      "115.0\n",
      "118.0\n",
      "115.0\n",
      "115.0\n",
      "119.0\n",
      "120.0\n",
      "115.0\n",
      "119.0\n",
      "113.0\n",
      "115.0\n",
      "112.0\n",
      "118.0\n",
      "117.0\n",
      "118.0\n",
      "119.0\n",
      "117.0\n",
      "116.0\n",
      "118.0\n",
      "121.0\n",
      "114.0\n",
      "123.0\n",
      "122.0\n",
      "119.0\n",
      "123.0\n",
      "114.0\n",
      "114.0\n",
      "Accuracy 0.9183\n"
     ]
    }
   ],
   "source": [
    "# modified Stanford Tensorflow class starter code from\n",
    "# from https://github.com/chiphuyen/stanford-tensorflow-tutorials/blob/master/2017/examples/03_logistic_regression_mnist_starter.py\n",
    "\n",
    "\"\"\" Starter code for logistic regression model to solve OCR task \n",
    "with MNIST in TensorFlow\n",
    "MNIST dataset: yann.lecun.com/exdb/mnist/\n",
    "Author: Chip Huyen\n",
    "Prepared for the class CS 20SI: \"TensorFlow for Deep Learning Research\"\n",
    "cs20si.stanford.edu\n",
    "\"\"\"\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL']='2'\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import time\n",
    "\n",
    "# Define paramaters for the model\n",
    "learning_rate = 0.01\n",
    "batch_size = 128\n",
    "n_epochs = 30\n",
    "\n",
    "# Step 1: Read in data\n",
    "# using TF Learn's built in function to load MNIST data to the folder data/mnist\n",
    "mnist = input_data.read_data_sets('data/mnist', one_hot=True) \n",
    "\n",
    "# Step 2: create placeholders for features and labels\n",
    "# each image in the MNIST data is of shape 28*28 = 784\n",
    "# therefore, each image is represented with a 1x784 tensor\n",
    "# there are 10 classes for each image, corresponding to digits 0 - 9. \n",
    "# Features are of the type float, and labels are of the type int\n",
    "\n",
    "\n",
    "X = tf.placeholder(tf.float32, [batch_size, 784], name=\"image\")\n",
    "Y = tf.placeholder(tf.float32, [batch_size, 10], name=\"label\")\n",
    "\n",
    "\n",
    "# Step 3: create weights and bias\n",
    "# weights and biases are initialized to 0\n",
    "# shape of w depends on the dimension of X and Y so that Y = X * w + b\n",
    "# shape of b depends on Y\n",
    "\n",
    "w = tf.Variable(tf.random_normal(shape=[784, 10], stddev=0.01), name=\"weights\")\n",
    "b = tf.Variable(tf.random_normal(shape=[1, 10]), name=\"biases\")\n",
    "\n",
    "\n",
    "# Step 4: build model\n",
    "# the model that returns the logits.\n",
    "# this logits will be later passed through softmax layer\n",
    "# to get the probability distribution of possible label of the image\n",
    "# DO NOT DO SOFTMAX HERE\n",
    "\n",
    "logits = tf.matmul(X, w) + b \n",
    "\n",
    "\n",
    "# Step 5: define loss function\n",
    "# use cross entropy loss of the real labels with the softmax of logits\n",
    "# use the method:\n",
    "# tf.nn.softmax_cross_entropy_with_logits(logits, Y)\n",
    "# then use tf.reduce_mean to get the mean loss of the batch\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=Y))\n",
    "\n",
    "# Step 6: define training op\n",
    "# using gradient descent to minimize loss\n",
    "train = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(loss)\n",
    "\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    loss_batch = 0 \n",
    "    start_time = time.time()\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    n_batches = int(mnist.train.num_examples/batch_size)\n",
    "    for i in range(n_epochs): # train the model n_epochs times\n",
    "        total_loss = 0\n",
    "        for _ in range(n_batches):\n",
    "            X_batch, Y_batch = mnist.train.next_batch(batch_size)\n",
    "    # TO-DO: run optimizer + fetch loss_batch\n",
    "            loss_batch, _ = sess.run([loss, train], feed_dict={X: X_batch, Y: Y_batch})\n",
    "            total_loss += loss_batch\n",
    "        print('Average loss epoch {0}: {1}'.format(i, total_loss/n_batches))\n",
    "\n",
    "    print('Total time: {0} seconds'.format(time.time() - start_time))\n",
    "\n",
    "    print('Optimization Finished!') # should be around 0.35 after 25 epochs\n",
    "\n",
    "    # test the model\n",
    "    preds = tf.nn.softmax(logits)\n",
    "    correct_preds = tf.equal(tf.argmax(preds, 1), tf.argmax(Y, 1))\n",
    "    accuracy = tf.reduce_sum(tf.cast(correct_preds, tf.float32)) # need numpy.count_nonzero(boolarr) :(\n",
    "    \n",
    "    n_batches = int(mnist.test.num_examples/batch_size)\n",
    "    total_correct_preds = 0\n",
    "    \n",
    "    for i in range(n_batches):\n",
    "        X_batch, Y_batch = mnist.test.next_batch(batch_size)\n",
    "        accuracy_batch = sess.run(accuracy, feed_dict={X: X_batch, Y:Y_batch}) \n",
    "        print(accuracy_batch)\n",
    "        total_correct_preds += accuracy_batch\n",
    "    \n",
    "    \n",
    "    print('Accuracy {0}'.format(total_correct_preds/mnist.test.num_examples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
